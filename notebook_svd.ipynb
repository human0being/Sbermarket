{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# from __future__ import (absolute_import, division, print_function,\n#                         unicode_literals)\nfrom collections import defaultdict\nfrom surprise import SVD\nfrom surprise import Dataset\nfrom surprise import Reader\nfrom surprise.model_selection import cross_validate\nfrom surprise.model_selection import train_test_split\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":3,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"from __future__ imports must occur at the beginning of the file (<ipython-input-3-ffd0e97c3391>, line 10)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-ffd0e97c3391>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    from surprise import SVD\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m from __future__ imports must occur at the beginning of the file\n"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\n\npath = r'../input/test-recsys/sbermarket_tab_2_1' # use your path\nall_files = glob.glob(path + \"/*.csv\")\n\nli = []\n\nfor filename in all_files:\n    df = pd.read_csv(filename, usecols=['user_id','product_id','quantity'])\n    li.append(df)\n\nframe1 = pd.concat(li, axis=0, ignore_index=True)\n\npath = r'../input/test-recsys/sbermarket_tab_2_2' # use your path\nall_files = glob.glob(path + \"/*.csv\")\n\nli = []\n\nfor filename in all_files:\n    df = pd.read_csv(filename, usecols=['user_id','product_id','quantity'])\n    li.append(df)\n\nframe2 = pd.concat(li, axis=0, ignore_index=True)\n\npath = r'../input/test-recsys/sbermarket_tab_2_3' # use your path\nall_files = glob.glob(path + \"/*.csv\")\n\nli = []\n\nfor filename in all_files:\n    df = pd.read_csv(filename, usecols=['user_id','product_id','quantity'])\n    li.append(df)\n\nframe3 = pd.concat(li, axis=0, ignore_index=True)\n\n# path = r'../input/test-recsys/sbermarket_tab_2_4' # use your path\n# all_files = glob.glob(path + \"/*.csv\")\n\n# li = []\n\n# for filename in all_files:\n#     df = pd.read_csv(filename, usecols=['user_id','product_id','quantity'])\n#     li.append(df)\n\n# frame4 = pd.concat(li, axis=0, ignore_index=True)\n\n# path = r'../input/test-recsys/sbermarket_tab_2_5' # use your path\n# all_files = glob.glob(path + \"/*.csv\")\n\n# li = []\n\n# for filename in all_files:\n#     df = pd.read_csv(filename, usecols=['user_id','product_id','quantity'])\n#     li.append(df)\n\n# frame5 = pd.concat(li, axis=0, ignore_index=True)\n\n# path = r'../input/test-recsys/sbermarket_tab_2_6' # use your path\n# all_files = glob.glob(path + \"/*.csv\")\n\n# li = []\n\n# for filename in all_files:\n#     df = pd.read_csv(filename, usecols=['user_id','product_id','quantity'])\n#     li.append(df)\n\n# frame6 = pd.concat(li, axis=0, ignore_index=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frame=pd.concat([frame1,frame2,frame3], axis=0, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neworder = ['user_id','product_id','quantity']\ndf=frame.reindex(columns=neworder)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del frame\ndel frame1\ndel frame2\ndel frame3\n#del frame4\n#del frame5\n#del frame6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install scikit-surprise","execution_count":4,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: scikit-surprise in /opt/conda/lib/python3.7/site-packages (1.1.1)\r\nRequirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-surprise) (1.4.1)\r\nRequirement already satisfied: numpy>=1.11.2 in /opt/conda/lib/python3.7/site-packages (from scikit-surprise) (1.18.5)\r\nRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from scikit-surprise) (1.14.0)\r\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-surprise) (0.14.1)\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"reader=Reader(rating_scale=(1, 5))","execution_count":5,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'Reader' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-a1debd8f246c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrating_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'Reader' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = Dataset.load_from_df(df, reader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set, test_set = train_test_split(data, test_size=0.2, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svd_algo=SVD()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svd_algo.fit(train_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=svd_algo.test(test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n(predictions, n=10):\n    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n    Args:\n        predictions(list of Prediction objects): The list of predictions, as\n            returned by the test method of an algorithm.\n        n(int): The number of recommendation to output for each user. Default\n            is 10.\n    Returns:\n    A dict where keys are user (raw) ids and values are lists of tuples:\n        [(raw item id, rating estimation), ...] of size n.\n    \"\"\"\n\n    # First map the predictions to each user.\n    top_n = defaultdict(list)\n    for uid, iid, true_r, est, _ in predictions:\n        top_n[uid].append((iid, est))\n\n    # Then sort the predictions for each user and retrieve the k highest ones.\n    for uid, user_ratings in top_n.items():\n        user_ratings.sort(key=lambda x: x[1], reverse=True)\n        top_n[uid] = user_ratings[:n]\n\n    return top_n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_pred=get_top_n(predictions, 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission=pd.read_csv('/kaggle/input/test-recsys/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a random sample dataframe\ndf = pd.DataFrame(np.random.randn(50, 4), columns=list('ABCD'))\n\n# create a link to download the dataframe\ncreate_download_link(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}